---
title: "432 final project"
author: "Mary Liu"
date: "4/1/2019"
output: pdf_document
---
```{r,echo=FALSE}
#read-in data
year1 = read.csv("/Users/maryliu/Downloads/data (1)/csv_result-1year.csv")
year2 = read.csv("/Users/maryliu/Downloads/data (1)/csv_result-2year.csv")
year3 = read.csv("/Users/maryliu/Downloads/data (1)/csv_result-3year.csv")
year4 = read.csv("/Users/maryliu/Downloads/data (1)/csv_result-4year.csv")
year5 = read.csv("/Users/maryliu/Downloads/data (1)/csv_result-5year.csv")
```

```{r,echo=FALSE}
#import library
library(ggplot2)
library(ggplot2)
library(reshape2)
library(caTools)
library(caret)
library(MASS)

library(ElemStatLearn)
library(mlbench)
library(glmnet)
library(e1071)

```

### Missing Values & Datat Preprocessing

We first conduct basic data preprocessing. Missing values for each dataset are shown in the graph below.


```{r, echo=FALSE}
year1[year1 == "?"] = NA
year2[year2 == "?"] = NA
year3[year3 == "?"] = NA
year4[year4 == "?"] = NA
year5[year5 == "?"] = NA

```

```{r, echo=FALSE}

num1 = sum(complete.cases(year1))
num2 = sum(complete.cases(year2))
num3 = sum(complete.cases(year3))
num4 = sum(complete.cases(year4))
num5 = sum(complete.cases(year5))
missing = data.frame(rbind(num1, num2, num3, num4, num5))
#missing = cbind(num1, num2, num3, num4, num5)
#colnames(missing) = c('year 1', 'year 2', 'year 3', 'year 4', 'year 5')
colnames(missing) = 'missing'
#rownames(missing) = 'missing values'

ggplot(missing, aes(x = c('year 1', 'year 2', 'year 3', 'year 4', 'year 5'), y=missing)) + geom_bar(stat="identity") + theme(axis.title.x = element_blank(),
  axis.title.y = element_blank())  + ggtitle("Missing values")
```


Due to the large number of missing values in each dataset, completely delete missing values will result to a large amount of data loss. Thus, we use variable means to replace missing values. We also drop the first variable `id` and factorize variable `class`.  We take year3 data as an example since it got largest missing value.

```{r}
asNumeric = function(x){
  
 as.numeric(as.character(x))
}

factorsNumeric = function(d){
  modifyList(d, lapply(d[, sapply(d, is.factor)],asNumeric))
}
year3 = factorsNumeric(year3)

for(i in 2:ncol(year3)){
  year3[is.na(year3[,i]), i] <- mean(year3[,i], na.rm = TRUE)
}

# drop id and factorize class
year3$id = NULL
year3$class = as.factor(year3$class)
```

### Pie Charts 
```{r, echo=FALSE}
#install.packages("gridExtra")
library(gridExtra)
library(ggplot2)
draw = function(num1, num2){
  type <- c('0 Not Brankrupcy','1 Brankrupcy')
  nums <- c(num1,num2)
  df = data.frame(type = type, nums = nums)
  p <- ggplot(data = df, mapping = aes(x = 'Content', y = nums, fill = type)) + geom_bar(stat   = 'identity', position = 'stack', width = 1)

  label_value = paste('(', round(df$nums/sum(df$nums) * 100, 1), '%)', sep = '')
  label = paste(df$type, label_value, sep = '')
  p + coord_polar(theta = 'y') + labs(x = '', y = '', title = '') + theme(axis.text =           element_blank()) + theme(axis.ticks = element_blank()) + scale_fill_discrete(labels = label)
}
par(mfrow=c(2,3))

p3 = draw(table(year3$class)[1],table(year3$class)[2]) + ggtitle("Year3")


grid.arrange(p3, nrow = 3)
```

The pie charts above show that the data is imbalanced. It has `0` with above 95.3%.   

###  Heatmap
```{r, echo=FALSE}

#heatmap plot year3
temp3 = year3[-65]

cormat <- round(cor(temp3),2)
melted_cormat <- melt(cormat)

  # Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }


upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed() + ggtitle("Year3")
# Print the heatmap


ggheatmap + 
theme(axis.text.x = element_text(size=4),
      axis.text.y = element_text(size=4),
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5)) +
  scale_y_discrete(position = "right")



```


#### LDA

Linear discriminate analysis is classifying observations in training data to the class with the closest centroid (with respect to Mahalanois distance), which also attains the highest posteriror density. We fit LDA model on `class` with respect to all other 64 predictors using `lda()`. 


```{r, echo=FALSE}
set.seed(123)
grid = expand.grid(k = 1:10)
knn_fit3 <- train(train.x_3,train.y_3,method = "knn", trControl = trainControl(method = "cv", 3,sampling = "smote"), tuneGrid = grid)
train_3_resample <- SMOTE(class ~ ., train_3, perc.over = 100, perc.under=200)
lda_fit3 = lda(class ~., data = train_3_resample)

```


#### SVM
SVM is a supervised learing models. It mapped the data as points in the space so that we can separate the data. In our case, we use radial kernel for nonlinear classification.

```{r, warning=FALSE}
set.seed(123)
trctrl = trainControl(method = "repeatedcv", number = 10, repeats = 3, sampling = "smote")

```
```{r, echo=FALSE}

svm_fit_3 = train(class ~., data = train_3, method = "svmRadial", trControl=trctrl, preProcess = c("center", "scale"), tuneLength = 10)

```
